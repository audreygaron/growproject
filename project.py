# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vzw_bPzdtjkeW97HUGmdKu4kdKH_jTN-
"""

import os
import torch
import torchvision
#import tarfile
import zipfile
from torchvision.datasets.utils import download_url
from torch.utils.data import random_split
from torchvision.datasets import ImageFolder
from torchvision.transforms import ToTensor, transforms
from torch.utils.data.dataloader import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary

"""# **Loading Dataset**

"""

batch_size = 16
device = torch.device("cuda" if torch.cuda.is_available() else torch.device("cpu"))
print("We're using =>", device)

from google.colab import drive
drive.mount('/content/gdrive')

!unzip /content/gdrive/MyDrive/GROW/dataset_images.zip

data_dir = 'dataset_images'

print(os.listdir(data_dir))
classes = os.listdir(data_dir + "/train")
print(classes)

#checking the number of images in each sub directory
robot_files = os.listdir(data_dir + '/train/robot')
print('Number of training examples for robot:', len(robot_files))
print(robot_files[:5])

not_robot_files = os.listdir(data_dir + '/train/not-robot')
print('Number of training examples for not robot:', len(not_robot_files))
print(not_robot_files[:5])

# converts images to tensors
transforms = transforms.Compose([transforms.Resize(32), transforms.CenterCrop(32), transforms.ToTensor()])
dataset = ImageFolder(data_dir+'/train', transform = transforms)
img, label = dataset[0]
print(img.shape, label)
print(dataset.classes)

# to get the same random sample each time (so if there are bugs we can run tests)
random_seed = 42
torch.manual_seed(random_seed)

val_size = int(len(dataset)*0.15)
train_size = len(dataset) - val_size

train_ds, val_ds = random_split(dataset, [train_size, val_size])
len(train_ds), len(val_ds)

test_ds = ImageFolder(data_dir+'/test', transform = transforms)

"""# **Create Dataset**"""

train_dl = DataLoader(train_ds, batch_size, shuffle=True)
val_dl = DataLoader(val_ds, batch_size, shuffle=True)
test_dl = DataLoader(test_ds, batch_size, shuffle=True )

def imshow(img):
  npimg = img.numpy()
  # We transpose here because the image has the shape [height, width, num_of_channel] but pytorch requires the shape to be [num_of_channel, 
  # height, width]
  plt.imshow(np.transpose(npimg, (1,2,0)))
  plt.show()

# get random training images
dataiter = iter(train_dl)
images, labels = dataiter.next()

# show the images in a grid
imshow(torchvision.utils.make_grid(images))
# print their labels
print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))

# the model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.block1 = self.conv_block(c_in=3, c_out=256, dropout=0.1, kernel_size=3, stride=1)
        self.block2 = self.conv_block(c_in=256, c_out=128, dropout=0.1, kernel_size=3, stride=1, padding=1)
        self.block3 = self.conv_block(c_in=128, c_out=64, dropout=0.1, kernel_size=3, stride=1, padding=1)
        self.lastcnn = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=3, stride=1, padding=0)
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.last = nn.Sequential(nn.Flatten(), 
            nn.Linear(2*5*5, 2))
    def forward(self, x):
        x = self.block1(x)
        x = self.maxpool(x)
        # print(x.shape)
        x = self.block2(x)
        # print(x.shape)
        x = self.block3(x)
        # print(x.shape)
        x = self.maxpool(x)
        # print(x.shape)
        x = self.lastcnn(x)
        # print(x.shape)
        x = self.last(x)
        # print(x.shape)
        return x
    def conv_block(self, c_in, c_out, dropout,  **kwargs):
        seq_block = nn.Sequential(
            nn.Conv2d(in_channels=c_in, out_channels=c_out, **kwargs),
            nn.BatchNorm2d(num_features=c_out),
            nn.ReLU(),
            nn.Dropout2d(p=dropout)
        )
        return seq_block
net = Net()
net.to(device)

summary(net.to(device), (3,32,32))

# define loss function & optimizer --> where we compute loss by comparing groundtruth with desired results
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
# creates optimizer --> the model's parameters tell the optimizer which weights to adjust where lr is the learning rate
optimizer = optim.Adam(net.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 2, gamma = 0.9)

# creates dictionaries to store accuracies
accuracy_stats = {
    'train': [],
    'val': []
}

loss_stats = {
    'train': [],
    'val': []
}

# computes accuracy
def binary_acc(y_pred, y_test):
  # look at the labels for the images
  y_pred_tag = torch.log_softmax(y_pred, dim = 1)
  _, y_pred_tags = torch.max(y_pred_tag, dim = 1)
  # add up all the correct results (1 if correct, 0 if wrong)
  correct_results_sum = (y_pred_tags == y_test).sum().float()
  # divide number of correct results by the total number of predictions
  acc = correct_results_sum / y_test.shape[0]
  acc = torch.round(acc * 100)
  return acc

from tqdm.notebook import tqdm
    
for e in tqdm(range(1, 15)):
  training_loss = 0
  training_accuracy = 0
  net.train()
  # data are the images we're currently looking at (aka the batch) and i is the step we're currently on in the epoch (iteration)
  for input_training_batch, label_training_batch in train_dl:
    input_training_batch, label_training_batch = input_training_batch.to(device), label_training_batch.to(device)
    optimizer.zero_grad()

    output_training_pred = net(input_training_batch).squeeze()
    train_loss = criterion(output_training_pred, label_training_batch)

    train_loss.backward()
    optimizer.step()

    train_accuracy = binary_acc(output_training_pred, label_training_batch)
    training_loss += train_loss.item()
    training_accuracy += train_accuracy.item()
  
  #validation 
  with torch.no_grad():
    net.eval()
    val_epoch_loss = 0
    val_epoch_accuracy = 0
    for input_val_batch, label_val_batch in val_dl:
      input_val_batch, label_val_batch = input_val_batch.to(device), label_val_batch.to(device)
      val_pred = net(input_val_batch).squeeze()
      val_loss = criterion(val_pred, label_val_batch)
      val_accuracy = binary_acc(val_pred, label_val_batch)
      val_epoch_loss += val_loss.item()
      val_epoch_accuracy += val_accuracy.item()
        
  scheduler.step()
  print('Epoch-{0} lr: {1}'.format(e, optimizer.param_groups[0]['lr']))
  loss_stats['train'].append(training_loss/len(train_dl))
  loss_stats['val'].append(val_epoch_loss/len(val_dl))
  accuracy_stats['train'].append(training_accuracy/len(train_dl))
  accuracy_stats['val'].append(val_epoch_accuracy/len(val_dl))
  print(f'Epoch {e+0:02}: | Train Loss: {training_loss/len(train_dl):.5f} | Val Loss: {val_epoch_loss/len(val_dl):.5f} | Train Acc: {training_accuracy/len(train_dl):.3f}| Val Acc: {val_epoch_accuracy/len(val_dl):.3f}')

# save the trained model
PATH = './cifar_net.pth'
torch.save(net.state_dict(), PATH)

dataiter = iter(test_dl)

images, labels = dataiter.next()

imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))

net = Net()
net.load_state_dict(torch.load(PATH))

outputs = net(images)

_, predicted = torch.max(outputs, 1)
print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(batch_size)))

correct = 0
total = 0

with torch.no_grad():
  for data in test_dl:
    images, labels = data
    y_pred = net(images)
    y_pred_tag = torch.log_softmax(y_pred, dim = 1)
    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)
    total += labels.size(0)
    correct += (y_pred_tags == labels).sum().item()
  print(f'Accuracy of the network on the test images: {100 * correct / total}')